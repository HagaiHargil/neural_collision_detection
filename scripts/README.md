# Pipeline Scripts Overview

This file explains the pipeline, the usage of each script file in this directory and a step by step guide to running the pipeline.


## Pipeline Overview
	Original data, represented by centers + radii (matlab db)
	|
	| Matlab code
	|
	V
	Triangle mesh
	|
	| Running ncd on pre-selected centers
	|
	V
	Set of locations with zero/very few collisions [1]
	|
	| Original data + run_aggregator.py + aggregator.py + gather_agg_results.py
	|
	V
	DB of more accurate collision count, with control of distance threshold [2]
	|
	| neuron_parser.py
	|
	V
	Array of overall collision count per voxel (over neurons)

[1] - Good point for visualization, using collisions_to_cubes.py + verify_zeros.py

[2] - Good point to make sure we are correct, since we calculate collisions with two different methods


## Scripts Usage
### Main scripts:
	aggregator.py
		Usage: aggregator.py <vascular data> <neuron data> <location> <rotation> <results file> [threshold distance]
		Receives vascular and neural data, and calculates manually the collisions/proximity sites for the given position.
		Outputs the results to 'results file'.

	run_aggregator.py
		Usage: run_aggregator.py <base dir> <max collisions> <threshold distance> <out dir>
		Runs aggregator.py for every position with <max collisions> collisions, found by ncd

	gather_agg_results.py
		Usage: gather_agg_results.py <input directory> <output file>
		Gather the results of aggregator.py into a single db (csv file).
		Columns are: run_id, neuron_id, vascular_id, neuron_location, neuron_rotation, collisions

	neuron_parser.py
		Usage: neuron_parser.py <input file> <output directory>
		Gets a db generated by gather_agg_results.py. Calculate an array of collisions per voxel over all positions.
		It also outputs some statistics to output_directory.

	run_ncd.sh
		Usage: run_ncd.sh. Should run from results directory.
		Runs ncd on all the neurons, in batch mode.


### Utils:
	create_cube.py
		Usage: create_cube.py <output file> <radius> <location>
		Creates an .obj file, with a single cube of given radius (half edge size) and location.
	extend.py
			extend.py <input filename> <output filename>
			Extends Centers.csv to have more centers, so we have more data. Used only for R&D.
	collisions_to_cubes.py
		Usage: collisions_to_cubes.py <input file> <output dir>
		Receives a list of collisions (locations), and creates multiple .obj files, each represent a collision as a cube.
		Used to visualize collisions on a neuorn/blood vessel.
	find_enclosing_box.py
		Usage: find_enclosing_box.py <object>
		Gets an object (.csv/.obj file), and outputs its bounding box. Used for debugging, and as a utility by other scripts.
	verify_zeros.py
		Usage: verify_zeros.py <base dir>
		Runs ncd in verify mode on each position with zero collisions, for debugging purposes.


## Blender:
	overlay_collisions.py
		Takes a loaded Blender neuron and overlays the collisions data on top of it for display purposes.
		
### Other:
	plotter.py
		Plots a general 2D array, using matplotlib. Not really needed right now.
	plotter_3d.py
		Just an example from the web. Not really needed right now.
	parser.py
		No usage, shouldn't be run as a standalone tool.


## Step By Step
1. Convert original representaions to .obj representations, using Matlab (TODO)

2. Run ncd on each neuron. An example for one neuron is:

	`./ncd -m batch -V ../vascular/vascular.obj -N ../neurons/AP120410_s1c1.obj -t 24 -o ncd_results/out1 -f ncd_results/out1.txt -i ../Centers.csv -z`

	For the next steps, make sure the output of all neurons is in the same root directory (`ncd_results` in this case)

3. Run aggregator and create aggregator_db.csv:
	For each desired threshold and max collision count, run (in this example, max 30 collisions and threshold of 2 microns):
	1. `python run_aggregator.py ../../results/ncd_results/ 30 2 ../../results/agg_results_30_2`

	2. `python gather_agg_results.py ../../results/agg_results_30_2 ../../results/aggregator_db.csv`
		Note that the results are appended to agregator_db.csv, so it will store all of the results together

4. Parse the DB:
	`python neuron_parser.py ../../results/aggregator_db.csv ../../results/parser_results`
