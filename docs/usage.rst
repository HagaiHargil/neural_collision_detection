General Pipeline Overview
=========================

Please keep in mind that this is the pipeline as it was run with our data and our goals in mind.

::

	Original data, represented by centers + radii (matlab db)
	|
	| Matlab code
	|
	V
	Triangle mesh (generates .obj files)
	|
	| Running the ncd binary on pre-selected centers
	|
	V
	Set of locations with zero/very few collisions which we now verify using brute force
	|
	| run_aggregator.py + db_to_dataframe.py
	|
	V
    Alpha Shape values
    |
    |   alpha_shapes_cgal.py
    |
    V
	DB of more accurate collision count, with control of distance threshold
	|
	| graph_parsing.py
	|
	V
	Graph-based representation of a neuron with each node containing data from the pipeline


Preparing the data for ncd:
===========================

We used data from `this <https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4635925/>`_ excellent paper of the Staiger lab for the neurons, and the vasculature arrived from `here <https://www.nature.com/articles/nn.3426>`_. The computational pipeline requires ``.obj`` and ``.csv`` files, but Python (``src/csv_converter``) and MATLAB (``src/convert_obj_matlab``) scripts that can tranlate existing data between these and other formats are also available here.

Given our data inputs, the pre-processing pipeline worked as follows:

1. First run ``Neurolucida2Mat.m`` on an input ``.asc`` file to create
the basic ``.mat`` file.
2. Further processing is done on that ``.mat`` file with the function ``VectorizeNeuron.m``, which should be
run on each ``.mat`` file generated by ``Neurolucida2Mat.m``.
3. Next, we create ``.csv`` files containing the vertices and faces of each mesh. We do this by running the functions ``ctp2mesh.m`` and ``write_mesh_to_disk.m`` on the ``.mat`` files and serializing the output:

::
        for file = files_mat'
        [M_surf, M_caps, offsetXYZ] = ctp2mesh(file.name);
        write_mesh_to_disk(M_surf, M_caps, offsetXYZ, file.name);
        end
``batch_ctp2mesh2disk.m`` helps to run this step on multiple files.  Please note that if the data is the vascular data, the offset removal done by ``bsxfun`` in the ``vertices.csv`` write to disk is not needed.

4. The mesh is translated to the more commonly used .obj format::

        `python csv_to_obj.py <vertices.csv> <faces.csv> <output.obj>`

2. Run ncd on each neuron. An example for one neuron is::

	./ncd -m batch -V ../vascular/vascular.obj -N ../neurons/AP120410_s1c1.obj -t 24 -o ncd_results/out1 -f ncd_results/out1.txt -i ../Centers.csv -z

For the next steps, make sure the output of all neurons is in the same root directory (`ncd_results` in this case)

3. Run aggregator and create aggregator_db.csv. For each desired threshold and max collision count, run (in this example, max 30 collisions and threshold of 2 microns)::

   python run_aggregator.py ../../results/ncd_results/ 30 2 ../../results/agg_results_30_2


4. Parse the DB::

    python db_to_dataframe.py

This will create a `.npz` file containing the parsed collision data.

Usage of ncd
============
There are three possible modes to run ncd:
	- regular: Test a neuron in a specific location, with all rotations
	- batch: Test a neuron in multiple locations, with all rotations
	- verify: Test a neuron in a specific location and rotation, and get .obj files for visualization

The base command line is as follows::

	./ncd -m <mode> -V <vascular_path> -N <neuron_path> -o <output_directory>
		 [-t num_of_threads] [-c max_num_of_collisions] [-a main_axis]
		 [mode parameters]

mode parameters::

	regular:
		-f <output_file> -l <x,y,z> [-z] [-b]
	batch:
		-f <output_file> -i <input_location_file> [-z] [-b]
	verify:
		-r <x,y,z> -l <x,y,z>

Recommended params::

	vascular_path, neuron_path - paths to .obj files, from stage 2
	output_directory - directory name to store the results
	output_file - file name of the results file
	num_of_threads - 36 on stromboli server
	max_num_of_collisions - 200, but may change according to results
	main_axis - the neuron rotates around this axis 360 degrees. Default - z
	-l - the location of the center of the neuron, in format of x,y,z
	-r - the rotation of the neuron, in format of x,y,z
	-i - input file with the locations of the neuron
	-z - store only 10 minimal positions for each location [Recommended]
	-b - DON'T eliminate results with bound violation [NOT Recommended]

An example of the way we ran ``ncd`` can be found in ``src/run_ncd.sh``.

Characteristics of ncd
======================

4.1	Collisions computation

The collision computation itself is done using an open source library called
``fcl`` (Flexible Collision Library).

4.2	Running time

The running time is affected by several factors:

- The complexity of the original meshes
- The simplification factor (0 for our case)
- The server running ncd
- The amount of collisions requested

On one of our servers, perhaps with some simplification, the running is
between 10 minutes and 15 minutes. Take into account that it may vary
if the mentioned factors are changed.

4.3	Output file

The outputfile contains the number of collisions, per rotation.
For rotation ``(x0,y0,z0)``, it means a rotation of ``x0`` degrees around x-axis,
then ``y0`` degrees around y-axis and then ``z0`` degrees around z-axis


Scripts Usage
================

::

	aggregator.py
		Usage: aggregator.py <vascular data> <neuron data> <location> <rotation> <results file> [threshold distance]
		Receives vascular and neural data, and calculates manually the collisions/proximity sites for the given position.
		Outputs the results to 'results file'.

	run_aggregator.py
		Usage: run_aggregator.py <base dir> <max collisions> <threshold distance> <out dir>
		Runs aggregator.py for every position with <max collisions> collisions, found by ncd

	gather_agg_results.py
		Usage: gather_agg_results.py <input directory> <output file>
		Gather the results of aggregator.py into a single db (csv file).
		Columns are: run_id, neuron_id, vascular_id, neuron_location, neuron_rotation, collisions

	neuron_parser.py
		Usage: neuron_parser.py <input file> <output directory>
		Gets a db generated by gather_agg_results.py. Calculate an array of collisions per voxel over all positions.
		It also outputs some statistics to output_directory.

	run_ncd.sh
		Usage: run_ncd.sh. Should run from results directory.
		Runs ncd on all the neurons, in batch mode.


Utils:
======

::

	create_cube.py
		Usage: create_cube.py <output file> <radius> <location>
		Creates an .obj file, with a single cube of given radius (half edge size) and location.
	extend.py
			extend.py <input filename> <output filename>
			Extends Centers.csv to have more centers, so we have more data. Used only for R&D.
	collisions_to_cubes.py
		Usage: collisions_to_cubes.py <input file> <output dir>
		Receives a list of collisions (locations), and creates multiple .obj files, each represent a collision as a cube.
		Used to visualize collisions on a neuorn/blood vessel.
	find_enclosing_box.py
		Usage: find_enclosing_box.py <object>
		Gets an object (.csv/.obj file), and outputs its bounding box. Used for debugging, and as a utility by other scripts.
	verify_zeros.py
		Usage: verify_zeros.py <base dir>
		Runs ncd in verify mode on each position with zero collisions, for debugging purposes.


Blender:
=======

::

    overlay_collisions.py
        Takes a loaded Blender neuron and overlays the collisions data on top of it for display purposes.
    draw_collisions_on_tree.py
        Assigns each collision detected through NCD to the neuronal tree it belongs to and renders it in 3D.

Other:
-----
::
	plotter.py
		Plots a general 2D array, using matplotlib. Not really needed right now.
	plotter_3d.py
		Just an example from the web. Not really needed right now.
	parser.py
		No usage, shouldn't be run as a standalone tool.

